{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# import torch.optim\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\matplotlib\\__init__.py:172\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\matplotlib\\rcsetup.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\matplotlib\\colors.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\matplotlib\\scale.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[0;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[0;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\matplotlib\\ticker.py:144\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[0;32m    146\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    148\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    150\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    157\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\matplotlib\\transforms.py:46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\numpy\\linalg\\__init__.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# To get sub-modules\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg  \u001b[38;5;66;03m# deprecated in NumPy 2.0\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _linalg\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     91\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m _linalg\u001b[38;5;241m.\u001b[39m__all__\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTuple, Any\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     array, asarray, zeros, empty, empty_like, intc, single, double,\n\u001b[0;32m     27\u001b[0m     csingle, cdouble, inexact, complexfloating, newaxis, \u001b[38;5;28mall\u001b[39m, inf, dot,\n\u001b[0;32m     28\u001b[0m     add, multiply, sqrt, \u001b[38;5;28msum\u001b[39m, isfinite, finfo, errstate, moveaxis, amin,\n\u001b[0;32m     29\u001b[0m     amax, prod, \u001b[38;5;28mabs\u001b[39m, atleast_2d, intp, asanyarray, object_, matmul,\n\u001b[0;32m     30\u001b[0m     swapaxes, divide, count_nonzero, isnan, sign, argsort, sort,\n\u001b[0;32m     31\u001b[0m     reciprocal, overrides, diagonal \u001b[38;5;28;01mas\u001b[39;00m _core_diagonal, trace \u001b[38;5;28;01mas\u001b[39;00m _core_trace,\n\u001b[0;32m     32\u001b[0m     cross \u001b[38;5;28;01mas\u001b[39;00m _core_cross, outer \u001b[38;5;28;01mas\u001b[39;00m _core_outer, tensordot \u001b[38;5;28;01mas\u001b[39;00m _core_tensordot,\n\u001b[0;32m     33\u001b[0m     matmul \u001b[38;5;28;01mas\u001b[39;00m _core_matmul, matrix_transpose \u001b[38;5;28;01mas\u001b[39;00m _core_matrix_transpose,\n\u001b[0;32m     34\u001b[0m     transpose \u001b[38;5;28;01mas\u001b[39;00m _core_transpose, vecdot \u001b[38;5;28;01mas\u001b[39;00m _core_vecdot,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_globals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _NoValue\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_twodim_base_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m triu, eye\n",
      "File \u001b[1;32md:\\conda\\envs\\pytorch\\Lib\\site-packages\\numpy\\_core\\overrides.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[0;32m     12\u001b[0m ARRAY_FUNCTIONS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     14\u001b[0m array_function_like_doc \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"like : array_like, optional\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m        Reference object to allow the creation of arrays which are not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m        compatible with that passed in via this argument.\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import random as r\n",
    "import copy\n",
    "from BFdataMaker import *\n",
    "import time\n",
    "from pretty_confusion_matrix import pp_matrix_from_data\n",
    "# 示例用法\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from time import *\n",
    "\n",
    "test_df = pd.read_csv(\"t1000.csv\")\n",
    "\n",
    "def evaluate(path, model_name, num_trees=500, depth=30, num_jobs=1):\n",
    "    df = pd.read_csv(path)\n",
    "    y = df.values[:,0]\n",
    "    x = df.values[:,1:]\n",
    "\n",
    "    test_y = test_df.values[:,0]\n",
    "    test_x = test_df.values[:,1:]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=num_trees, max_depth=depth, n_jobs=num_jobs)\n",
    "    start = time()\n",
    "    rf.fit(x, y)\n",
    "    end = time()\n",
    "    elapsed = end - start\n",
    "    print(\"Time to train model %s: %.9f seconds\" % (model_name, elapsed))\n",
    "\n",
    "    acc = np.mean(test_y == rf.predict(test_x))\n",
    "    print(\"Model %s accuracy: %.3f\" % (model_name, acc))\n",
    "\n",
    "evaluate(\"t10k.csv\", \"10k\", 500, 10, 48)    # choose your own parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 层\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # 计算注意力权重\n",
    "        attention_scores = self.W(input1 + input2)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # 加权融合特征\n",
    "        output = attention_weights * input1 + (1 - attention_weights) * input2\n",
    "\n",
    "        return output\n",
    "\n",
    "# 假设input1和input2是两类输入数据\n",
    "input1 = torch.randn(10, 50)  # 10个样本，每个样本50维特征\n",
    "input2 = torch.randn(10, 50)\n",
    "\n",
    "# 创建注意力模块\n",
    "attention = Attention(input_dim=50)\n",
    "\n",
    "# 融合特征\n",
    "output = attention(input1, input2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_time = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=5,kernel_size=9,padding=4), \n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5,5,kernel_size=2,stride=2), # (40,40)\n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.model_corp = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10,out_channels=10,kernel_size=7,padding = 3),\n",
    "            nn.BatchNorm2d(10,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,10,kernel_size=2,stride=2),\n",
    "            nn.BatchNorm2d(10,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=10,out_channels=15,kernel_size=5,padding = 2),\n",
    "            nn.BatchNorm2d(15,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(15,15,kernel_size=2,stride=2),\n",
    "            nn.BatchNorm2d(15,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=15,out_channels=30,kernel_size=3,padding = 1),\n",
    "            nn.BatchNorm2d(30,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(30,30,kernel_size=2,stride=2),\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(750,750),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(750,3),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        self.model_fre = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=5,kernel_size=6), \n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=5,out_channels=5,kernel_size=6), \n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "      \n",
    "\n",
    "    def forward(self, input_tot):\n",
    "        # print(input,input.shape)\n",
    "        input_time = input_tot[:,:6400]\n",
    "        input_fre = input_tot[:,6400:]\n",
    "        input_time = input_time.reshape(-1,1,80,80) \n",
    "        input_time = self.model_time(input_time)\n",
    "\n",
    "        input_fre = input_fre.reshape(-1,1,50,50)\n",
    "        input_fre = self.model_fre(input_fre)\n",
    "\n",
    "        input_corp = torch.cat((input_time,input_fre),dim=1)\n",
    "        input_corp = self.model_corp(input_corp)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        return input_corp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_current = torch.zeros(80,16,7,7)\n",
    "x_voltage = torch.zeros(80,1,7,7)\n",
    "print(x_current.shape, x_voltage.shape)\n",
    "fusing = torch.cat((x_current,x_voltage),dim=1)\n",
    "print(fusing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "print(summary(model,input_size=[(1,1, 80, 80), (1,1, 50, 50)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy import signal\n",
    "\n",
    "# 生成示例信号（假设采样率为Fs）\n",
    "Fs = 1000  # 采样率\n",
    "T = 1/Fs  # 采样间隔\n",
    "t = np.arange(0, 1, T)\n",
    "signal_freq = 50  # 信号频率\n",
    "signal_amplitude = 1\n",
    "signal_noise = 0.2 * np.random.randn(len(t))  # 添加噪音\n",
    "signal_data = signal_amplitude * np.sin(2 * np.pi * signal_freq * t) + signal_noise\n",
    "\n",
    "# 进行FFT\n",
    "signal_fft = fft(signal_data)\n",
    "\n",
    "# 获取频率轴\n",
    "freqs = np.fft.fftfreq(len(signal_data), T)\n",
    "print(freqs)\n",
    "# 识别主要频率分量\n",
    "main_freq_index = np.argmax(np.abs(signal_fft))\n",
    "main_freq = freqs[main_freq_index]\n",
    "print(main_freq)\n",
    "# 设计带阻滤波器\n",
    "notch_freq = np.abs(main_freq)  # 带阻滤波器中心频率即为主要频率\n",
    "Q = 10  # 带阻滤波器的质因数\n",
    "b, a = signal.iirnotch(notch_freq, Q, Fs)\n",
    "\n",
    "# 应用带阻滤波器\n",
    "filtered_signal_fft = signal.lfilter(b, a, signal_fft)\n",
    "\n",
    "# 逆FFT得到处理后的信号\n",
    "filtered_signal = ifft(filtered_signal_fft)\n",
    "\n",
    "# 可视化结果或进一步分析处理后的信号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N09_M07_F10_K001_1 raw_data\\K001\\K001\\N09_M07_F10_K001_1.mat\n",
      "N09_M07_F10_KA04_1 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_1.mat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n如果以6400个点做fft变换，因为采样率的原因，得到的频谱图的x轴的精度为10Hz，低于3000Hz成分只有300个点\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "如果以6400个点做fft变换，因为采样率的原因，得到的频谱图的x轴的精度为10Hz，低于3000Hz成分只有300个点\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_loader_from_1D_data(trainX,trainY,valX,valY):\n",
    "    trainX,valX = torch.FloatTensor(trainX),torch.FloatTensor(valX)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    trainY = encoder.fit_transform(trainY.ravel())\n",
    "    encoder = LabelEncoder()\n",
    "    valY = encoder.fit_transform(valY.ravel())\n",
    "    trainY,valY = torch.LongTensor(trainY),torch.LongTensor(valY)\n",
    "\n",
    "    train_dataset =  torch.utils.data.TensorDataset(trainX, trainY)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, )\n",
    "    test_dataset =  torch.utils.data.TensorDataset(valX, valY)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(valX), shuffle=True, )\n",
    "\n",
    "    return train_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据.pth创建模型实例\n",
    "model_state_dict = torch.load('best_model_essay_optim.pth',map_location=torch.device('cpu'))\n",
    "# \n",
    "model = Model()\n",
    "# 设置模型为评估模式\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()# 将模型参数加载到模型实例中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tem \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m tem \u001b[38;5;241m=\u001b[39m tem\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "tem = [1,2,3]\n",
    "tem = tem.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_process(model,train_dataloader,val_dataloader,num_epoches):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.99))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_acc = 0.0\n",
    "    train_loss_all = []\n",
    "    val_loss_all = []\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    since = time.time()\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1,num_epoches))\n",
    "        print(\"-\"*10)\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_corrects = 0\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "        total_G_mean = 0\n",
    "        y_pred = []  # 存储模型的预测结果\n",
    "        y_test = []  # 存储真实标签\n",
    "\n",
    "        for step, (bx,by) in enumerate(train_dataloader):\n",
    "\n",
    "            bx = bx.to(device)\n",
    "            by = by.to(device)\n",
    "\n",
    "            model.train()\n",
    "            output = model(bx)\n",
    "\n",
    "            # print(output)\n",
    "            pre_lab = torch.argmax(output,dim = 1)\n",
    "            print(pre_lab.type)\n",
    "            print(by.type)\n",
    "            y_pred.extend(pre_lab.numpy())  # 将预测结果添加到y_pred列表中\n",
    "            y_test.extend(by.numpy())  # 将真实标签添加到y_test列表中                \n",
    "\n",
    "            loss = criterion(output, by)\n",
    "            # print('realy:',by)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()* bx.size(0)\n",
    "            train_corrects += torch.sum(pre_lab == by.data)\n",
    "            train_num += bx.size(0)\n",
    "\n",
    "\n",
    "        columns = ['health', 'fault']\n",
    "        pp_matrix_from_data(np.array(y_test), np.array(y_pred), columns)  # 调用混淆矩阵可视化函数\n",
    "\n",
    "        train_loss_all.append(train_loss/train_num)\n",
    "        train_acc_all.append(train_corrects/train_num)\n",
    "        # if(epoch % 5)\n",
    "        y_pred = []  # 存储模型的预测结果\n",
    "        y_test = []  # 存储真实标签\n",
    "\n",
    "        total_G_mean = 0\n",
    "        for step, (bx,by) in enumerate(val_dataloader):\n",
    "\n",
    "            bx = bx.to(device)\n",
    "            by = by.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            output = model(bx)\n",
    "\n",
    "            pre_lab = torch.argmax(output,dim = 1)\n",
    "            loss = criterion(output, by)\n",
    "            y_pred.extend(pre_lab.cpu().numpy())  # 将预测结果添加到y_pred列表中\n",
    "            y_test.extend(by.cpu().numpy())  # 将真实标签添加到y_test列表中                \n",
    "\n",
    "            val_loss += loss.item() * bx.size(0)\n",
    "            val_corrects += torch.sum(pre_lab == by.data)\n",
    "\n",
    "            val_num += bx.size(0)\n",
    "\n",
    "            \n",
    "        \n",
    "        columns = ['health', 'fault']\n",
    "        pp_matrix_from_data(np.array(y_test), np.array(y_pred), columns)  # 调用混淆矩阵可视化函数\n",
    "\n",
    "\n",
    "        val_loss_all.append(val_loss/val_num)\n",
    "        val_acc_all.append(val_corrects/val_num)\n",
    "\n",
    "        print('Epoch {} Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch+1,train_loss_all[-1],train_acc_all[-1]))\n",
    "        print('Epoch {} Test Loss: {:.4f} Test Acc: {:.4f}'.format(epoch+1,val_loss_all[-1],val_acc_all[-1]))\n",
    "\n",
    "        if val_acc_all[-1] > best_acc:\n",
    "            best_acc = val_acc_all[-1]\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_use = time.time() - since\n",
    "        print(\"Time usage {:.0f}m{:.0f}s\".format(time_use//60,time_use%60))\n",
    "\n",
    "    torch.save(best_model_wts, \"best_model_essay_optim.pth\")\n",
    "    train_process = pd.DataFrame(data = {\"epoch\": range(num_epoches),\n",
    "                                        \"train_acc_all\": train_acc_all,\n",
    "                                        \"train_loss_all\": train_loss_all,\n",
    "                                        \"val_acc_all\": val_acc_all,\n",
    "                                        \"val_loss_all\": val_loss_all,\n",
    "                                         })\n",
    "    return train_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N09_M07_F10_K001_1 raw_data\\K001\\K001\\N09_M07_F10_K001_1.mat\n",
      "N09_M07_F10_K001_10 raw_data\\K001\\K001\\N09_M07_F10_K001_10.mat\n",
      "N09_M07_F10_K001_11 raw_data\\K001\\K001\\N09_M07_F10_K001_11.mat\n",
      "N09_M07_F10_K001_12 raw_data\\K001\\K001\\N09_M07_F10_K001_12.mat\n",
      "N09_M07_F10_K001_13 raw_data\\K001\\K001\\N09_M07_F10_K001_13.mat\n",
      "N09_M07_F10_K001_14 raw_data\\K001\\K001\\N09_M07_F10_K001_14.mat\n",
      "N09_M07_F10_K001_15 raw_data\\K001\\K001\\N09_M07_F10_K001_15.mat\n",
      "N09_M07_F10_K001_16 raw_data\\K001\\K001\\N09_M07_F10_K001_16.mat\n",
      "N09_M07_F10_K001_17 raw_data\\K001\\K001\\N09_M07_F10_K001_17.mat\n",
      "N09_M07_F10_K001_18 raw_data\\K001\\K001\\N09_M07_F10_K001_18.mat\n",
      "N09_M07_F10_K001_19 raw_data\\K001\\K001\\N09_M07_F10_K001_19.mat\n",
      "N09_M07_F10_K001_2 raw_data\\K001\\K001\\N09_M07_F10_K001_2.mat\n",
      "N09_M07_F10_K001_20 raw_data\\K001\\K001\\N09_M07_F10_K001_20.mat\n",
      "N09_M07_F10_K001_3 raw_data\\K001\\K001\\N09_M07_F10_K001_3.mat\n",
      "N09_M07_F10_K001_4 raw_data\\K001\\K001\\N09_M07_F10_K001_4.mat\n",
      "N09_M07_F10_K001_5 raw_data\\K001\\K001\\N09_M07_F10_K001_5.mat\n",
      "N09_M07_F10_K001_6 raw_data\\K001\\K001\\N09_M07_F10_K001_6.mat\n",
      "N09_M07_F10_K001_7 raw_data\\K001\\K001\\N09_M07_F10_K001_7.mat\n",
      "N09_M07_F10_K001_8 raw_data\\K001\\K001\\N09_M07_F10_K001_8.mat\n",
      "N09_M07_F10_K001_9 raw_data\\K001\\K001\\N09_M07_F10_K001_9.mat\n",
      "N15_M01_F10_K001_1 raw_data\\K001\\K001\\N15_M01_F10_K001_1.mat\n",
      "N15_M01_F10_K001_10 raw_data\\K001\\K001\\N15_M01_F10_K001_10.mat\n",
      "N15_M01_F10_K001_11 raw_data\\K001\\K001\\N15_M01_F10_K001_11.mat\n",
      "N15_M01_F10_K001_12 raw_data\\K001\\K001\\N15_M01_F10_K001_12.mat\n",
      "N15_M01_F10_K001_13 raw_data\\K001\\K001\\N15_M01_F10_K001_13.mat\n",
      "N15_M01_F10_K001_14 raw_data\\K001\\K001\\N15_M01_F10_K001_14.mat\n",
      "N15_M01_F10_K001_15 raw_data\\K001\\K001\\N15_M01_F10_K001_15.mat\n",
      "N15_M01_F10_K001_16 raw_data\\K001\\K001\\N15_M01_F10_K001_16.mat\n",
      "N15_M01_F10_K001_17 raw_data\\K001\\K001\\N15_M01_F10_K001_17.mat\n",
      "N15_M01_F10_K001_18 raw_data\\K001\\K001\\N15_M01_F10_K001_18.mat\n",
      "N15_M01_F10_K001_19 raw_data\\K001\\K001\\N15_M01_F10_K001_19.mat\n",
      "N15_M01_F10_K001_2 raw_data\\K001\\K001\\N15_M01_F10_K001_2.mat\n",
      "N15_M01_F10_K001_20 raw_data\\K001\\K001\\N15_M01_F10_K001_20.mat\n",
      "N15_M01_F10_K001_3 raw_data\\K001\\K001\\N15_M01_F10_K001_3.mat\n",
      "N15_M01_F10_K001_4 raw_data\\K001\\K001\\N15_M01_F10_K001_4.mat\n",
      "N15_M01_F10_K001_5 raw_data\\K001\\K001\\N15_M01_F10_K001_5.mat\n",
      "N15_M01_F10_K001_6 raw_data\\K001\\K001\\N15_M01_F10_K001_6.mat\n",
      "N15_M01_F10_K001_7 raw_data\\K001\\K001\\N15_M01_F10_K001_7.mat\n",
      "N15_M01_F10_K001_8 raw_data\\K001\\K001\\N15_M01_F10_K001_8.mat\n",
      "N15_M01_F10_K001_9 raw_data\\K001\\K001\\N15_M01_F10_K001_9.mat\n",
      "N15_M07_F04_K001_1 raw_data\\K001\\K001\\N15_M07_F04_K001_1.mat\n",
      "N15_M07_F04_K001_10 raw_data\\K001\\K001\\N15_M07_F04_K001_10.mat\n",
      "N15_M07_F04_K001_11 raw_data\\K001\\K001\\N15_M07_F04_K001_11.mat\n",
      "N15_M07_F04_K001_12 raw_data\\K001\\K001\\N15_M07_F04_K001_12.mat\n",
      "N15_M07_F04_K001_13 raw_data\\K001\\K001\\N15_M07_F04_K001_13.mat\n",
      "N15_M07_F04_K001_14 raw_data\\K001\\K001\\N15_M07_F04_K001_14.mat\n",
      "N15_M07_F04_K001_15 raw_data\\K001\\K001\\N15_M07_F04_K001_15.mat\n",
      "N15_M07_F04_K001_16 raw_data\\K001\\K001\\N15_M07_F04_K001_16.mat\n",
      "N15_M07_F04_K001_17 raw_data\\K001\\K001\\N15_M07_F04_K001_17.mat\n",
      "N15_M07_F04_K001_18 raw_data\\K001\\K001\\N15_M07_F04_K001_18.mat\n",
      "N15_M07_F04_K001_19 raw_data\\K001\\K001\\N15_M07_F04_K001_19.mat\n",
      "N15_M07_F04_K001_2 raw_data\\K001\\K001\\N15_M07_F04_K001_2.mat\n",
      "N15_M07_F04_K001_20 raw_data\\K001\\K001\\N15_M07_F04_K001_20.mat\n",
      "N15_M07_F04_K001_3 raw_data\\K001\\K001\\N15_M07_F04_K001_3.mat\n",
      "N15_M07_F04_K001_4 raw_data\\K001\\K001\\N15_M07_F04_K001_4.mat\n",
      "N15_M07_F04_K001_5 raw_data\\K001\\K001\\N15_M07_F04_K001_5.mat\n",
      "N15_M07_F04_K001_6 raw_data\\K001\\K001\\N15_M07_F04_K001_6.mat\n",
      "N15_M07_F04_K001_7 raw_data\\K001\\K001\\N15_M07_F04_K001_7.mat\n",
      "N15_M07_F04_K001_8 raw_data\\K001\\K001\\N15_M07_F04_K001_8.mat\n",
      "N15_M07_F04_K001_9 raw_data\\K001\\K001\\N15_M07_F04_K001_9.mat\n",
      "N15_M07_F10_K001_1 raw_data\\K001\\K001\\N15_M07_F10_K001_1.mat\n",
      "N15_M07_F10_K001_10 raw_data\\K001\\K001\\N15_M07_F10_K001_10.mat\n",
      "N15_M07_F10_K001_11 raw_data\\K001\\K001\\N15_M07_F10_K001_11.mat\n",
      "N15_M07_F10_K001_12 raw_data\\K001\\K001\\N15_M07_F10_K001_12.mat\n",
      "N15_M07_F10_K001_13 raw_data\\K001\\K001\\N15_M07_F10_K001_13.mat\n",
      "N15_M07_F10_K001_14 raw_data\\K001\\K001\\N15_M07_F10_K001_14.mat\n",
      "N15_M07_F10_K001_15 raw_data\\K001\\K001\\N15_M07_F10_K001_15.mat\n",
      "N15_M07_F10_K001_16 raw_data\\K001\\K001\\N15_M07_F10_K001_16.mat\n",
      "N15_M07_F10_K001_17 raw_data\\K001\\K001\\N15_M07_F10_K001_17.mat\n",
      "N15_M07_F10_K001_18 raw_data\\K001\\K001\\N15_M07_F10_K001_18.mat\n",
      "N15_M07_F10_K001_19 raw_data\\K001\\K001\\N15_M07_F10_K001_19.mat\n",
      "N15_M07_F10_K001_2 raw_data\\K001\\K001\\N15_M07_F10_K001_2.mat\n",
      "N15_M07_F10_K001_20 raw_data\\K001\\K001\\N15_M07_F10_K001_20.mat\n",
      "N15_M07_F10_K001_3 raw_data\\K001\\K001\\N15_M07_F10_K001_3.mat\n",
      "N15_M07_F10_K001_4 raw_data\\K001\\K001\\N15_M07_F10_K001_4.mat\n",
      "N15_M07_F10_K001_5 raw_data\\K001\\K001\\N15_M07_F10_K001_5.mat\n",
      "N15_M07_F10_K001_6 raw_data\\K001\\K001\\N15_M07_F10_K001_6.mat\n",
      "N15_M07_F10_K001_7 raw_data\\K001\\K001\\N15_M07_F10_K001_7.mat\n",
      "N15_M07_F10_K001_8 raw_data\\K001\\K001\\N15_M07_F10_K001_8.mat\n",
      "N15_M07_F10_K001_9 raw_data\\K001\\K001\\N15_M07_F10_K001_9.mat\n",
      "N09_M07_F10_KA04_1 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_1.mat\n",
      "N09_M07_F10_KA04_10 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_10.mat\n",
      "N09_M07_F10_KA04_11 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_11.mat\n",
      "N09_M07_F10_KA04_12 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_12.mat\n",
      "N09_M07_F10_KA04_13 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_13.mat\n",
      "N09_M07_F10_KA04_14 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_14.mat\n",
      "N09_M07_F10_KA04_15 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_15.mat\n",
      "N09_M07_F10_KA04_16 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_16.mat\n",
      "N09_M07_F10_KA04_17 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_17.mat\n",
      "N09_M07_F10_KA04_18 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_18.mat\n",
      "N09_M07_F10_KA04_19 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_19.mat\n",
      "N09_M07_F10_KA04_2 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_2.mat\n",
      "N09_M07_F10_KA04_20 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_20.mat\n",
      "N09_M07_F10_KA04_3 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_3.mat\n",
      "N09_M07_F10_KA04_4 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_4.mat\n",
      "N09_M07_F10_KA04_5 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_5.mat\n",
      "N09_M07_F10_KA04_6 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_6.mat\n",
      "N09_M07_F10_KA04_7 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_7.mat\n",
      "N09_M07_F10_KA04_8 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_8.mat\n",
      "N09_M07_F10_KA04_9 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_9.mat\n",
      "N15_M01_F10_KA04_1 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_1.mat\n",
      "N15_M01_F10_KA04_10 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_10.mat\n",
      "N15_M01_F10_KA04_11 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_11.mat\n",
      "N15_M01_F10_KA04_12 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_12.mat\n",
      "N15_M01_F10_KA04_13 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_13.mat\n",
      "N15_M01_F10_KA04_14 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_14.mat\n",
      "N15_M01_F10_KA04_15 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_15.mat\n",
      "N15_M01_F10_KA04_16 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_16.mat\n",
      "N15_M01_F10_KA04_17 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_17.mat\n",
      "N15_M01_F10_KA04_18 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_18.mat\n",
      "N15_M01_F10_KA04_19 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_19.mat\n",
      "N15_M01_F10_KA04_2 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_2.mat\n",
      "N15_M01_F10_KA04_20 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_20.mat\n",
      "N15_M01_F10_KA04_3 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_3.mat\n",
      "N15_M01_F10_KA04_4 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_4.mat\n",
      "N15_M01_F10_KA04_5 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_5.mat\n",
      "N15_M01_F10_KA04_6 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_6.mat\n",
      "N15_M01_F10_KA04_7 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_7.mat\n",
      "N15_M01_F10_KA04_8 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_8.mat\n",
      "N15_M01_F10_KA04_9 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_9.mat\n",
      "N15_M07_F04_KA04_1 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_1.mat\n",
      "N15_M07_F04_KA04_10 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_10.mat\n",
      "N15_M07_F04_KA04_11 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_11.mat\n",
      "N15_M07_F04_KA04_12 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_12.mat\n",
      "N15_M07_F04_KA04_13 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_13.mat\n",
      "N15_M07_F04_KA04_14 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_14.mat\n",
      "N15_M07_F04_KA04_15 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_15.mat\n",
      "N15_M07_F04_KA04_16 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_16.mat\n",
      "N15_M07_F04_KA04_17 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_17.mat\n",
      "N15_M07_F04_KA04_18 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_18.mat\n",
      "N15_M07_F04_KA04_19 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_19.mat\n",
      "N15_M07_F04_KA04_2 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_2.mat\n",
      "N15_M07_F04_KA04_20 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_20.mat\n",
      "N15_M07_F04_KA04_3 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_3.mat\n",
      "N15_M07_F04_KA04_4 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_4.mat\n",
      "N15_M07_F04_KA04_5 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_5.mat\n",
      "N15_M07_F04_KA04_6 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_6.mat\n",
      "N15_M07_F04_KA04_7 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_7.mat\n",
      "N15_M07_F04_KA04_8 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_8.mat\n",
      "N15_M07_F04_KA04_9 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_9.mat\n",
      "N15_M07_F10_KA04_1 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_1.mat\n",
      "N15_M07_F10_KA04_10 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_10.mat\n",
      "N15_M07_F10_KA04_11 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_11.mat\n",
      "N15_M07_F10_KA04_12 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_12.mat\n",
      "N15_M07_F10_KA04_13 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_13.mat\n",
      "N15_M07_F10_KA04_14 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_14.mat\n",
      "N15_M07_F10_KA04_15 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_15.mat\n",
      "N15_M07_F10_KA04_16 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_16.mat\n",
      "N15_M07_F10_KA04_17 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_17.mat\n",
      "N15_M07_F10_KA04_18 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_18.mat\n",
      "N15_M07_F10_KA04_19 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_19.mat\n",
      "N15_M07_F10_KA04_2 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_2.mat\n",
      "N15_M07_F10_KA04_20 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_20.mat\n",
      "N15_M07_F10_KA04_3 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_3.mat\n",
      "N15_M07_F10_KA04_4 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_4.mat\n",
      "N15_M07_F10_KA04_5 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_5.mat\n",
      "N15_M07_F10_KA04_6 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_6.mat\n",
      "N15_M07_F10_KA04_7 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_7.mat\n",
      "N15_M07_F10_KA04_8 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_8.mat\n",
      "N15_M07_F10_KA04_9 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_9.mat\n",
      "Getting data with keyword K001\n",
      "Getting data with keyword KA04\n",
      "(6158,) (6158, 8900)\n",
      "(536,) (536, 8900)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "datadicts = {'K001':0,'KA04':1}\n",
    "# for each in datadicts:\n",
    "#     get_data_and_convert_to_pixels(each,datadicts[each],6400,6400,128000,output_dir=\"G_lp_td_fd_pixel\")\n",
    "data_foldername= \"G_lp_td_fd_pixel\"\n",
    "sample_length = 8901\n",
    "trainX,trainY,valX,valY = load_data_and_convert_to_grid(datadicts,0.92,data_foldername,sample_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader = get_two_loader_from_1D_data(trainX,trainY,valX,valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model_process(model,train_loader,test_loader,\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_model_process' is not defined"
     ]
    }
   ],
   "source": [
    "train_model_process(model,train_loader,test_loader,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

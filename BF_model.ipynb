{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import random as r\n",
    "import copy\n",
    "from BFdataMaker import *\n",
    "import time\n",
    "\n",
    "# 示例用法\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from time import *\n",
    "\n",
    "test_df = pd.read_csv(\"t1000.csv\")\n",
    "\n",
    "def evaluate(path, model_name, num_trees=500, depth=30, num_jobs=1):\n",
    "    df = pd.read_csv(path)\n",
    "    y = df.values[:,0]\n",
    "    x = df.values[:,1:]\n",
    "\n",
    "    test_y = test_df.values[:,0]\n",
    "    test_x = test_df.values[:,1:]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=num_trees, max_depth=depth, n_jobs=num_jobs)\n",
    "    start = time()\n",
    "    rf.fit(x, y)\n",
    "    end = time()\n",
    "    elapsed = end - start\n",
    "    print(\"Time to train model %s: %.9f seconds\" % (model_name, elapsed))\n",
    "\n",
    "    acc = np.mean(test_y == rf.predict(test_x))\n",
    "    print(\"Model %s accuracy: %.3f\" % (model_name, acc))\n",
    "\n",
    "evaluate(\"t10k.csv\", \"10k\", 500, 10, 48)    # choose your own parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 层\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # 计算注意力权重\n",
    "        attention_scores = self.W(input1 + input2)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # 加权融合特征\n",
    "        output = attention_weights * input1 + (1 - attention_weights) * input2\n",
    "\n",
    "        return output\n",
    "\n",
    "# 假设input1和input2是两类输入数据\n",
    "input1 = torch.randn(10, 50)  # 10个样本，每个样本50维特征\n",
    "input2 = torch.randn(10, 50)\n",
    "\n",
    "# 创建注意力模块\n",
    "attention = Attention(input_dim=50)\n",
    "\n",
    "# 融合特征\n",
    "output = attention(input1, input2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_time = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=5,kernel_size=9,padding=4), \n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5,5,kernel_size=2,stride=2), # (40,40)\n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.model_corp = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10,out_channels=10,kernel_size=7,padding = 3),\n",
    "            nn.BatchNorm2d(10,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10,10,kernel_size=2,stride=2),\n",
    "            nn.BatchNorm2d(10,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=10,out_channels=15,kernel_size=5,padding = 2),\n",
    "            nn.BatchNorm2d(15,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(15,15,kernel_size=2,stride=2),\n",
    "            nn.BatchNorm2d(15,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=15,out_channels=30,kernel_size=3,padding = 1),\n",
    "            nn.BatchNorm2d(30,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(30,30,kernel_size=2,stride=2),\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(750,750),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(750,2),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        self.model_fre = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=5,kernel_size=6), \n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=5,out_channels=5,kernel_size=6), \n",
    "            nn.BatchNorm2d(5,1e-6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "      \n",
    "\n",
    "    def forward(self, input_tot):\n",
    "        # print(input,input.shape)\n",
    "        input_time = input_tot[:,:6400]\n",
    "        input_fre = input_tot[:,6400:]\n",
    "        input_time = input_time.reshape(-1,1,80,80)   #结果为[128,1,21]  目的是把二维变为三维数据\n",
    "        input_time = self.model_time(input_time)\n",
    "\n",
    "        input_fre = input_fre.reshape(-1,1,50,50)\n",
    "        input_fre = self.model_fre(input_fre)\n",
    "\n",
    "        input_corp = torch.cat((input_time,input_fre),dim=1)\n",
    "        input_corp = self.model_corp(input_corp)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        return input_corp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 16, 7, 7]) torch.Size([80, 1, 7, 7])\n",
      "torch.Size([80, 17, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x_current = torch.zeros(80,16,7,7)\n",
    "x_voltage = torch.zeros(80,1,7,7)\n",
    "print(x_current.shape, x_voltage.shape)\n",
    "fusing = torch.cat((x_current,x_voltage),dim=1)\n",
    "print(fusing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary(model,input_size\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m80\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m)]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(summary(model,input_size=[(1,1, 80, 80), (1,1, 50, 50)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data with keyword K001\n",
      "Getting data with keyword KA04\n",
      "(11752,) (11752, 80, 80)\n",
      "(1022,) (1022, 80, 80)\n"
     ]
    }
   ],
   "source": [
    "datadicts = {'health':0,'bpfi':1,'bpfo':2}\n",
    "# for i in datadicts:\n",
    "#     get_8080_one_line_current_data_change_to_pixel_with_label_from_keyword(i,datadicts[i])\n",
    "\n",
    "# %%\n",
    "# datadicts = {'K001':0,'K004':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy import signal\n",
    "\n",
    "# 生成示例信号（假设采样率为Fs）\n",
    "Fs = 1000  # 采样率\n",
    "T = 1/Fs  # 采样间隔\n",
    "t = np.arange(0, 1, T)\n",
    "signal_freq = 50  # 信号频率\n",
    "signal_amplitude = 1\n",
    "signal_noise = 0.2 * np.random.randn(len(t))  # 添加噪音\n",
    "signal_data = signal_amplitude * np.sin(2 * np.pi * signal_freq * t) + signal_noise\n",
    "\n",
    "# 进行FFT\n",
    "signal_fft = fft(signal_data)\n",
    "\n",
    "# 获取频率轴\n",
    "freqs = np.fft.fftfreq(len(signal_data), T)\n",
    "print(freqs)\n",
    "# 识别主要频率分量\n",
    "main_freq_index = np.argmax(np.abs(signal_fft))\n",
    "main_freq = freqs[main_freq_index]\n",
    "print(main_freq)\n",
    "# 设计带阻滤波器\n",
    "notch_freq = np.abs(main_freq)  # 带阻滤波器中心频率即为主要频率\n",
    "Q = 10  # 带阻滤波器的质因数\n",
    "b, a = signal.iirnotch(notch_freq, Q, Fs)\n",
    "\n",
    "# 应用带阻滤波器\n",
    "filtered_signal_fft = signal.lfilter(b, a, signal_fft)\n",
    "\n",
    "# 逆FFT得到处理后的信号\n",
    "filtered_signal = ifft(filtered_signal_fft)\n",
    "\n",
    "# 可视化结果或进一步分析处理后的信号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N09_M07_F10_K001_1 raw_data\\K001\\K001\\N09_M07_F10_K001_1.mat\n",
      "N09_M07_F10_KA04_1 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_1.mat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n如果以6400个点做fft变换，因为采样率的原因，得到的频谱图的x轴的精度为10Hz，低于3000Hz成分只有300个点\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "如果以6400个点做fft变换，因为采样率的原因，得到的频谱图的x轴的精度为10Hz，低于3000Hz成分只有300个点\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_loader_from_1D_data(trainX,trainY,valX,valY):\n",
    "    trainX,valX = torch.FloatTensor(trainX),torch.FloatTensor(valX)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    trainY = encoder.fit_transform(trainY.ravel())\n",
    "    encoder = LabelEncoder()\n",
    "    valY = encoder.fit_transform(valY.ravel())\n",
    "    trainY,valY = torch.LongTensor(trainY),torch.LongTensor(valY)\n",
    "\n",
    "    train_dataset =  torch.utils.data.TensorDataset(trainX, trainY)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, )\n",
    "    test_dataset =  torch.utils.data.TensorDataset(valX, valY)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(valX), shuffle=True, )\n",
    "\n",
    "    return train_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据.pth创建模型实例\n",
    "model_state_dict = torch.load('best_model_essay_optim.pth',map_location=torch.device('cpu'))\n",
    "# \n",
    "model = Model()\n",
    "# 设置模型为评估模式\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()# 将模型参数加载到模型实例中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_process(model,train_dataloader,val_dataloader,num_epoches):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.99))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_acc = 0.0\n",
    "    train_loss_all = []\n",
    "    val_loss_all = []\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    since = time.time()\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1,num_epoches))\n",
    "        print(\"-\"*10)\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_corrects = 0\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        train_num = 0\n",
    "        val_num = 0\n",
    "        total_G_mean = 0\n",
    "\n",
    "        for step, (bx,by) in enumerate(train_dataloader):\n",
    "\n",
    "            bx = bx.to(device)\n",
    "            by = by.to(device)\n",
    "\n",
    "            model.train()\n",
    "            output = model(bx)\n",
    "\n",
    "            # print(output)\n",
    "            pre_lab = torch.argmax(output,dim = 1)\n",
    "            # print(pre_lab)\n",
    "            loss = criterion(output, by)\n",
    "            # print('realy:',by)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()* bx.size(0)\n",
    "            train_corrects += torch.sum(pre_lab == by.data)\n",
    "            train_num += bx.size(0)\n",
    "\n",
    "            matrix = confusion_matrix(by.data, pre_lab)\n",
    "            # print(Y_data,matrix.shape)\n",
    "            if(matrix.shape != (2,2)):\n",
    "                continue\n",
    "            TN = matrix[0][0]\n",
    "            FP = matrix[0][1]\n",
    "            FN = matrix[1][0]\n",
    "            TP = matrix[1][1]\n",
    "\n",
    "            FDR = TP / (TP + FN)\n",
    "            FAR = FP / (FP + TN)\n",
    "            P = TN / (TN + FP)\n",
    "            G_mean = math.sqrt(FDR * P)\n",
    "            if np.isnan(G_mean):\n",
    "                G_mean = 0.0\n",
    "\n",
    "            total_G_mean = total_G_mean + G_mean\n",
    "            \n",
    "        length = len(train_dataloader)\n",
    "        total_G_mean = total_G_mean / length\n",
    "        print(\"Epoch {} Train G-mean为{:.4f}\".format(epoch+1,total_G_mean))\n",
    "\n",
    "        train_loss_all.append(train_loss/train_num)\n",
    "        train_acc_all.append(train_corrects/train_num)\n",
    "        # if(epoch % 5)\n",
    "\n",
    "        total_G_mean = 0\n",
    "        for step, (bx,by) in enumerate(val_dataloader):\n",
    "\n",
    "            bx = bx.to(device)\n",
    "            by = by.to(device)\n",
    "\n",
    "            model.eval()\n",
    "            output = model(bx)\n",
    "\n",
    "            pre_lab = torch.argmax(output,dim = 1)\n",
    "            loss = criterion(output, by)\n",
    "\n",
    "            val_loss += loss.item() * bx.size(0)\n",
    "            val_corrects += torch.sum(pre_lab == by.data)\n",
    "\n",
    "            val_num += bx.size(0)\n",
    "\n",
    "            matrix = confusion_matrix(by.data, pre_lab)\n",
    "            # print(Y_data,matrix.shape)\n",
    "            if(matrix.shape != (2,2)):\n",
    "                continue\n",
    "            TN = matrix[0][0]\n",
    "            FP = matrix[0][1]\n",
    "            FN = matrix[1][0]\n",
    "            TP = matrix[1][1]\n",
    "\n",
    "            FDR = TP / (TP + FN)\n",
    "            FAR = FP / (FP + TN)\n",
    "            P = TN / (TN + FP)\n",
    "            G_mean = math.sqrt(FDR * P)\n",
    "            if np.isnan(G_mean):\n",
    "                G_mean = 0.0\n",
    "\n",
    "            total_G_mean = total_G_mean + G_mean\n",
    "        \n",
    "        length = len(val_dataloader)\n",
    "        total_G_mean = total_G_mean / length\n",
    "        print(\"Epoch {} Test G-mean为{:.4f}\".format(epoch+1,total_G_mean))\n",
    "\n",
    "        val_loss_all.append(val_loss/val_num)\n",
    "        val_acc_all.append(val_corrects/val_num)\n",
    "\n",
    "        print('Epoch {} Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch+1,train_loss_all[-1],train_acc_all[-1]))\n",
    "        print('Epoch {} Test Loss: {:.4f} Test Acc: {:.4f}'.format(epoch+1,val_loss_all[-1],val_acc_all[-1]))\n",
    "\n",
    "        if val_acc_all[-1] > best_acc:\n",
    "            best_acc = val_acc_all[-1]\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_use = time.time() - since\n",
    "        print(\"Time usage {:.0f}m{:.0f}s\".format(time_use//60,time_use%60))\n",
    "\n",
    "    torch.save(best_model_wts, \"best_model_essay_optim.pth\")\n",
    "    train_process = pd.DataFrame(data = {\"epoch\": range(num_epoches),\n",
    "                                        \"train_acc_all\": train_acc_all,\n",
    "                                        \"train_loss_all\": train_loss_all,\n",
    "                                        \"val_acc_all\": val_acc_all,\n",
    "                                        \"val_loss_all\": val_loss_all,\n",
    "                                         })\n",
    "    return train_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N09_M07_F10_K001_1 raw_data\\K001\\K001\\N09_M07_F10_K001_1.mat\n",
      "N09_M07_F10_K001_10 raw_data\\K001\\K001\\N09_M07_F10_K001_10.mat\n",
      "N09_M07_F10_K001_11 raw_data\\K001\\K001\\N09_M07_F10_K001_11.mat\n",
      "N09_M07_F10_K001_12 raw_data\\K001\\K001\\N09_M07_F10_K001_12.mat\n",
      "N09_M07_F10_K001_13 raw_data\\K001\\K001\\N09_M07_F10_K001_13.mat\n",
      "N09_M07_F10_K001_14 raw_data\\K001\\K001\\N09_M07_F10_K001_14.mat\n",
      "N09_M07_F10_K001_15 raw_data\\K001\\K001\\N09_M07_F10_K001_15.mat\n",
      "N09_M07_F10_K001_16 raw_data\\K001\\K001\\N09_M07_F10_K001_16.mat\n",
      "N09_M07_F10_K001_17 raw_data\\K001\\K001\\N09_M07_F10_K001_17.mat\n",
      "N09_M07_F10_K001_18 raw_data\\K001\\K001\\N09_M07_F10_K001_18.mat\n",
      "N09_M07_F10_K001_19 raw_data\\K001\\K001\\N09_M07_F10_K001_19.mat\n",
      "N09_M07_F10_K001_2 raw_data\\K001\\K001\\N09_M07_F10_K001_2.mat\n",
      "N09_M07_F10_K001_20 raw_data\\K001\\K001\\N09_M07_F10_K001_20.mat\n",
      "N09_M07_F10_K001_3 raw_data\\K001\\K001\\N09_M07_F10_K001_3.mat\n",
      "N09_M07_F10_K001_4 raw_data\\K001\\K001\\N09_M07_F10_K001_4.mat\n",
      "N09_M07_F10_K001_5 raw_data\\K001\\K001\\N09_M07_F10_K001_5.mat\n",
      "N09_M07_F10_K001_6 raw_data\\K001\\K001\\N09_M07_F10_K001_6.mat\n",
      "N09_M07_F10_K001_7 raw_data\\K001\\K001\\N09_M07_F10_K001_7.mat\n",
      "N09_M07_F10_K001_8 raw_data\\K001\\K001\\N09_M07_F10_K001_8.mat\n",
      "N09_M07_F10_K001_9 raw_data\\K001\\K001\\N09_M07_F10_K001_9.mat\n",
      "N15_M01_F10_K001_1 raw_data\\K001\\K001\\N15_M01_F10_K001_1.mat\n",
      "N15_M01_F10_K001_10 raw_data\\K001\\K001\\N15_M01_F10_K001_10.mat\n",
      "N15_M01_F10_K001_11 raw_data\\K001\\K001\\N15_M01_F10_K001_11.mat\n",
      "N15_M01_F10_K001_12 raw_data\\K001\\K001\\N15_M01_F10_K001_12.mat\n",
      "N15_M01_F10_K001_13 raw_data\\K001\\K001\\N15_M01_F10_K001_13.mat\n",
      "N15_M01_F10_K001_14 raw_data\\K001\\K001\\N15_M01_F10_K001_14.mat\n",
      "N15_M01_F10_K001_15 raw_data\\K001\\K001\\N15_M01_F10_K001_15.mat\n",
      "N15_M01_F10_K001_16 raw_data\\K001\\K001\\N15_M01_F10_K001_16.mat\n",
      "N15_M01_F10_K001_17 raw_data\\K001\\K001\\N15_M01_F10_K001_17.mat\n",
      "N15_M01_F10_K001_18 raw_data\\K001\\K001\\N15_M01_F10_K001_18.mat\n",
      "N15_M01_F10_K001_19 raw_data\\K001\\K001\\N15_M01_F10_K001_19.mat\n",
      "N15_M01_F10_K001_2 raw_data\\K001\\K001\\N15_M01_F10_K001_2.mat\n",
      "N15_M01_F10_K001_20 raw_data\\K001\\K001\\N15_M01_F10_K001_20.mat\n",
      "N15_M01_F10_K001_3 raw_data\\K001\\K001\\N15_M01_F10_K001_3.mat\n",
      "N15_M01_F10_K001_4 raw_data\\K001\\K001\\N15_M01_F10_K001_4.mat\n",
      "N15_M01_F10_K001_5 raw_data\\K001\\K001\\N15_M01_F10_K001_5.mat\n",
      "N15_M01_F10_K001_6 raw_data\\K001\\K001\\N15_M01_F10_K001_6.mat\n",
      "N15_M01_F10_K001_7 raw_data\\K001\\K001\\N15_M01_F10_K001_7.mat\n",
      "N15_M01_F10_K001_8 raw_data\\K001\\K001\\N15_M01_F10_K001_8.mat\n",
      "N15_M01_F10_K001_9 raw_data\\K001\\K001\\N15_M01_F10_K001_9.mat\n",
      "N15_M07_F04_K001_1 raw_data\\K001\\K001\\N15_M07_F04_K001_1.mat\n",
      "N15_M07_F04_K001_10 raw_data\\K001\\K001\\N15_M07_F04_K001_10.mat\n",
      "N15_M07_F04_K001_11 raw_data\\K001\\K001\\N15_M07_F04_K001_11.mat\n",
      "N15_M07_F04_K001_12 raw_data\\K001\\K001\\N15_M07_F04_K001_12.mat\n",
      "N15_M07_F04_K001_13 raw_data\\K001\\K001\\N15_M07_F04_K001_13.mat\n",
      "N15_M07_F04_K001_14 raw_data\\K001\\K001\\N15_M07_F04_K001_14.mat\n",
      "N15_M07_F04_K001_15 raw_data\\K001\\K001\\N15_M07_F04_K001_15.mat\n",
      "N15_M07_F04_K001_16 raw_data\\K001\\K001\\N15_M07_F04_K001_16.mat\n",
      "N15_M07_F04_K001_17 raw_data\\K001\\K001\\N15_M07_F04_K001_17.mat\n",
      "N15_M07_F04_K001_18 raw_data\\K001\\K001\\N15_M07_F04_K001_18.mat\n",
      "N15_M07_F04_K001_19 raw_data\\K001\\K001\\N15_M07_F04_K001_19.mat\n",
      "N15_M07_F04_K001_2 raw_data\\K001\\K001\\N15_M07_F04_K001_2.mat\n",
      "N15_M07_F04_K001_20 raw_data\\K001\\K001\\N15_M07_F04_K001_20.mat\n",
      "N15_M07_F04_K001_3 raw_data\\K001\\K001\\N15_M07_F04_K001_3.mat\n",
      "N15_M07_F04_K001_4 raw_data\\K001\\K001\\N15_M07_F04_K001_4.mat\n",
      "N15_M07_F04_K001_5 raw_data\\K001\\K001\\N15_M07_F04_K001_5.mat\n",
      "N15_M07_F04_K001_6 raw_data\\K001\\K001\\N15_M07_F04_K001_6.mat\n",
      "N15_M07_F04_K001_7 raw_data\\K001\\K001\\N15_M07_F04_K001_7.mat\n",
      "N15_M07_F04_K001_8 raw_data\\K001\\K001\\N15_M07_F04_K001_8.mat\n",
      "N15_M07_F04_K001_9 raw_data\\K001\\K001\\N15_M07_F04_K001_9.mat\n",
      "N15_M07_F10_K001_1 raw_data\\K001\\K001\\N15_M07_F10_K001_1.mat\n",
      "N15_M07_F10_K001_10 raw_data\\K001\\K001\\N15_M07_F10_K001_10.mat\n",
      "N15_M07_F10_K001_11 raw_data\\K001\\K001\\N15_M07_F10_K001_11.mat\n",
      "N15_M07_F10_K001_12 raw_data\\K001\\K001\\N15_M07_F10_K001_12.mat\n",
      "N15_M07_F10_K001_13 raw_data\\K001\\K001\\N15_M07_F10_K001_13.mat\n",
      "N15_M07_F10_K001_14 raw_data\\K001\\K001\\N15_M07_F10_K001_14.mat\n",
      "N15_M07_F10_K001_15 raw_data\\K001\\K001\\N15_M07_F10_K001_15.mat\n",
      "N15_M07_F10_K001_16 raw_data\\K001\\K001\\N15_M07_F10_K001_16.mat\n",
      "N15_M07_F10_K001_17 raw_data\\K001\\K001\\N15_M07_F10_K001_17.mat\n",
      "N15_M07_F10_K001_18 raw_data\\K001\\K001\\N15_M07_F10_K001_18.mat\n",
      "N15_M07_F10_K001_19 raw_data\\K001\\K001\\N15_M07_F10_K001_19.mat\n",
      "N15_M07_F10_K001_2 raw_data\\K001\\K001\\N15_M07_F10_K001_2.mat\n",
      "N15_M07_F10_K001_20 raw_data\\K001\\K001\\N15_M07_F10_K001_20.mat\n",
      "N15_M07_F10_K001_3 raw_data\\K001\\K001\\N15_M07_F10_K001_3.mat\n",
      "N15_M07_F10_K001_4 raw_data\\K001\\K001\\N15_M07_F10_K001_4.mat\n",
      "N15_M07_F10_K001_5 raw_data\\K001\\K001\\N15_M07_F10_K001_5.mat\n",
      "N15_M07_F10_K001_6 raw_data\\K001\\K001\\N15_M07_F10_K001_6.mat\n",
      "N15_M07_F10_K001_7 raw_data\\K001\\K001\\N15_M07_F10_K001_7.mat\n",
      "N15_M07_F10_K001_8 raw_data\\K001\\K001\\N15_M07_F10_K001_8.mat\n",
      "N15_M07_F10_K001_9 raw_data\\K001\\K001\\N15_M07_F10_K001_9.mat\n",
      "N09_M07_F10_KA04_1 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_1.mat\n",
      "N09_M07_F10_KA04_10 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_10.mat\n",
      "N09_M07_F10_KA04_11 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_11.mat\n",
      "N09_M07_F10_KA04_12 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_12.mat\n",
      "N09_M07_F10_KA04_13 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_13.mat\n",
      "N09_M07_F10_KA04_14 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_14.mat\n",
      "N09_M07_F10_KA04_15 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_15.mat\n",
      "N09_M07_F10_KA04_16 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_16.mat\n",
      "N09_M07_F10_KA04_17 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_17.mat\n",
      "N09_M07_F10_KA04_18 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_18.mat\n",
      "N09_M07_F10_KA04_19 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_19.mat\n",
      "N09_M07_F10_KA04_2 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_2.mat\n",
      "N09_M07_F10_KA04_20 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_20.mat\n",
      "N09_M07_F10_KA04_3 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_3.mat\n",
      "N09_M07_F10_KA04_4 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_4.mat\n",
      "N09_M07_F10_KA04_5 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_5.mat\n",
      "N09_M07_F10_KA04_6 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_6.mat\n",
      "N09_M07_F10_KA04_7 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_7.mat\n",
      "N09_M07_F10_KA04_8 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_8.mat\n",
      "N09_M07_F10_KA04_9 raw_data\\KA04\\KA04\\N09_M07_F10_KA04_9.mat\n",
      "N15_M01_F10_KA04_1 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_1.mat\n",
      "N15_M01_F10_KA04_10 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_10.mat\n",
      "N15_M01_F10_KA04_11 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_11.mat\n",
      "N15_M01_F10_KA04_12 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_12.mat\n",
      "N15_M01_F10_KA04_13 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_13.mat\n",
      "N15_M01_F10_KA04_14 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_14.mat\n",
      "N15_M01_F10_KA04_15 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_15.mat\n",
      "N15_M01_F10_KA04_16 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_16.mat\n",
      "N15_M01_F10_KA04_17 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_17.mat\n",
      "N15_M01_F10_KA04_18 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_18.mat\n",
      "N15_M01_F10_KA04_19 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_19.mat\n",
      "N15_M01_F10_KA04_2 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_2.mat\n",
      "N15_M01_F10_KA04_20 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_20.mat\n",
      "N15_M01_F10_KA04_3 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_3.mat\n",
      "N15_M01_F10_KA04_4 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_4.mat\n",
      "N15_M01_F10_KA04_5 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_5.mat\n",
      "N15_M01_F10_KA04_6 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_6.mat\n",
      "N15_M01_F10_KA04_7 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_7.mat\n",
      "N15_M01_F10_KA04_8 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_8.mat\n",
      "N15_M01_F10_KA04_9 raw_data\\KA04\\KA04\\N15_M01_F10_KA04_9.mat\n",
      "N15_M07_F04_KA04_1 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_1.mat\n",
      "N15_M07_F04_KA04_10 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_10.mat\n",
      "N15_M07_F04_KA04_11 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_11.mat\n",
      "N15_M07_F04_KA04_12 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_12.mat\n",
      "N15_M07_F04_KA04_13 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_13.mat\n",
      "N15_M07_F04_KA04_14 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_14.mat\n",
      "N15_M07_F04_KA04_15 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_15.mat\n",
      "N15_M07_F04_KA04_16 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_16.mat\n",
      "N15_M07_F04_KA04_17 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_17.mat\n",
      "N15_M07_F04_KA04_18 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_18.mat\n",
      "N15_M07_F04_KA04_19 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_19.mat\n",
      "N15_M07_F04_KA04_2 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_2.mat\n",
      "N15_M07_F04_KA04_20 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_20.mat\n",
      "N15_M07_F04_KA04_3 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_3.mat\n",
      "N15_M07_F04_KA04_4 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_4.mat\n",
      "N15_M07_F04_KA04_5 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_5.mat\n",
      "N15_M07_F04_KA04_6 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_6.mat\n",
      "N15_M07_F04_KA04_7 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_7.mat\n",
      "N15_M07_F04_KA04_8 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_8.mat\n",
      "N15_M07_F04_KA04_9 raw_data\\KA04\\KA04\\N15_M07_F04_KA04_9.mat\n",
      "N15_M07_F10_KA04_1 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_1.mat\n",
      "N15_M07_F10_KA04_10 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_10.mat\n",
      "N15_M07_F10_KA04_11 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_11.mat\n",
      "N15_M07_F10_KA04_12 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_12.mat\n",
      "N15_M07_F10_KA04_13 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_13.mat\n",
      "N15_M07_F10_KA04_14 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_14.mat\n",
      "N15_M07_F10_KA04_15 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_15.mat\n",
      "N15_M07_F10_KA04_16 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_16.mat\n",
      "N15_M07_F10_KA04_17 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_17.mat\n",
      "N15_M07_F10_KA04_18 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_18.mat\n",
      "N15_M07_F10_KA04_19 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_19.mat\n",
      "N15_M07_F10_KA04_2 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_2.mat\n",
      "N15_M07_F10_KA04_20 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_20.mat\n",
      "N15_M07_F10_KA04_3 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_3.mat\n",
      "N15_M07_F10_KA04_4 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_4.mat\n",
      "N15_M07_F10_KA04_5 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_5.mat\n",
      "N15_M07_F10_KA04_6 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_6.mat\n",
      "N15_M07_F10_KA04_7 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_7.mat\n",
      "N15_M07_F10_KA04_8 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_8.mat\n",
      "N15_M07_F10_KA04_9 raw_data\\KA04\\KA04\\N15_M07_F10_KA04_9.mat\n",
      "Getting data with keyword K001\n",
      "Getting data with keyword KA04\n",
      "(6158,) (6158, 8900)\n",
      "(536,) (536, 8900)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "datadicts = {'K001':0,'KA04':1}\n",
    "for each in datadicts:\n",
    "    get_data_and_convert_to_pixels(each,datadicts[each],6400,6400,128000,output_dir=\"G_lp_td_fd_pixel\")\n",
    "data_foldername= \"G_lp_td_fd_pixel\"\n",
    "sample_length = 8901\n",
    "trainX,trainY,valX,valY = load_data_and_convert_to_grid(datadicts,0.92,data_foldername,sample_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n",
      "Epoch 1 Train G-mean为0.5457\n",
      "Epoch 1 Test G-mean为0.0000\n",
      "Epoch 1 Train Loss: 0.6767 Train Acc: 0.5771\n",
      "Epoch 1 Test Loss: 0.7890 Test Acc: 0.5000\n",
      "Time usage 0m44s\n",
      "Epoch 2/20\n",
      "----------\n",
      "Epoch 2 Train G-mean为0.7153\n",
      "Epoch 2 Test G-mean为0.1058\n",
      "Epoch 2 Train Loss: 0.5791 Train Acc: 0.7140\n",
      "Epoch 2 Test Loss: 0.8038 Test Acc: 0.5056\n",
      "Time usage 1m29s\n",
      "Epoch 3/20\n",
      "----------\n",
      "Epoch 3 Train G-mean为0.7694\n",
      "Epoch 3 Test G-mean为0.7570\n",
      "Epoch 3 Train Loss: 0.5237 Train Acc: 0.7714\n",
      "Epoch 3 Test Loss: 0.5240 Test Acc: 0.7743\n",
      "Time usage 2m15s\n",
      "Epoch 4/20\n",
      "----------\n",
      "Epoch 4 Train G-mean为0.8068\n",
      "Epoch 4 Test G-mean为0.6377\n",
      "Epoch 4 Train Loss: 0.4970 Train Acc: 0.8058\n",
      "Epoch 4 Test Loss: 0.5935 Test Acc: 0.7034\n",
      "Time usage 3m43s\n",
      "Epoch 5/20\n",
      "----------\n",
      "Epoch 5 Train G-mean为0.8165\n",
      "Epoch 5 Test G-mean为0.6366\n",
      "Epoch 5 Train Loss: 0.4837 Train Acc: 0.8197\n",
      "Epoch 5 Test Loss: 0.5899 Test Acc: 0.7015\n",
      "Time usage 4m26s\n",
      "Epoch 6/20\n",
      "----------\n",
      "Epoch 6 Train G-mean为0.8269\n",
      "Epoch 6 Test G-mean为0.7452\n",
      "Epoch 6 Train Loss: 0.4825 Train Acc: 0.8243\n",
      "Epoch 6 Test Loss: 0.5159 Test Acc: 0.7705\n",
      "Time usage 5m10s\n",
      "Epoch 7/20\n",
      "----------\n",
      "Epoch 7 Train G-mean为0.8492\n",
      "Epoch 7 Test G-mean为0.8373\n",
      "Epoch 7 Train Loss: 0.4647 Train Acc: 0.8478\n",
      "Epoch 7 Test Loss: 0.4741 Test Acc: 0.8377\n",
      "Time usage 5m55s\n",
      "Epoch 8/20\n",
      "----------\n",
      "Epoch 8 Train G-mean为0.8539\n",
      "Epoch 8 Test G-mean为0.8232\n",
      "Epoch 8 Train Loss: 0.4580 Train Acc: 0.8543\n",
      "Epoch 8 Test Loss: 0.4719 Test Acc: 0.8246\n",
      "Time usage 6m41s\n",
      "Epoch 9/20\n",
      "----------\n",
      "Epoch 9 Train G-mean为0.8534\n",
      "Epoch 9 Test G-mean为0.8035\n",
      "Epoch 9 Train Loss: 0.4562 Train Acc: 0.8571\n",
      "Epoch 9 Test Loss: 0.4957 Test Acc: 0.8134\n",
      "Time usage 7m21s\n",
      "Epoch 10/20\n",
      "----------\n",
      "Epoch 10 Train G-mean为0.8736\n",
      "Epoch 10 Test G-mean为0.7628\n",
      "Epoch 10 Train Loss: 0.4452 Train Acc: 0.8750\n",
      "Epoch 10 Test Loss: 0.5173 Test Acc: 0.7836\n",
      "Time usage 8m3s\n",
      "Epoch 11/20\n",
      "----------\n",
      "Epoch 11 Train G-mean为0.8916\n",
      "Epoch 11 Test G-mean为0.8089\n",
      "Epoch 11 Train Loss: 0.4298 Train Acc: 0.8959\n",
      "Epoch 11 Test Loss: 0.4758 Test Acc: 0.8246\n",
      "Time usage 8m42s\n",
      "Epoch 12/20\n",
      "----------\n",
      "Epoch 12 Train G-mean为0.8892\n",
      "Epoch 12 Test G-mean为0.7495\n",
      "Epoch 12 Train Loss: 0.4278 Train Acc: 0.8910\n",
      "Epoch 12 Test Loss: 0.5127 Test Acc: 0.7761\n",
      "Time usage 9m29s\n",
      "Epoch 13/20\n",
      "----------\n",
      "Epoch 13 Train G-mean为0.9161\n",
      "Epoch 13 Test G-mean为0.5305\n",
      "Epoch 13 Train Loss: 0.4087 Train Acc: 0.9139\n",
      "Epoch 13 Test Loss: 0.6468 Test Acc: 0.6381\n",
      "Time usage 10m14s\n",
      "Epoch 14/20\n",
      "----------\n",
      "Epoch 14 Train G-mean为0.9269\n",
      "Epoch 14 Test G-mean为0.6794\n",
      "Epoch 14 Train Loss: 0.4013 Train Acc: 0.9235\n",
      "Epoch 14 Test Loss: 0.5714 Test Acc: 0.7220\n",
      "Time usage 10m57s\n",
      "Epoch 15/20\n",
      "----------\n",
      "Epoch 15 Train G-mean为0.9397\n",
      "Epoch 15 Test G-mean为0.8327\n",
      "Epoch 15 Train Loss: 0.3860 Train Acc: 0.9411\n",
      "Epoch 15 Test Loss: 0.4600 Test Acc: 0.8377\n",
      "Time usage 11m47s\n",
      "Epoch 16/20\n",
      "----------\n",
      "Epoch 16 Train G-mean为0.9479\n",
      "Epoch 16 Test G-mean为0.9308\n",
      "Epoch 16 Train Loss: 0.3809 Train Acc: 0.9459\n",
      "Epoch 16 Test Loss: 0.3983 Test Acc: 0.9310\n",
      "Time usage 12m32s\n",
      "Epoch 17/20\n",
      "----------\n",
      "Epoch 17 Train G-mean为0.9508\n",
      "Epoch 17 Test G-mean为0.8991\n",
      "Epoch 17 Train Loss: 0.3679 Train Acc: 0.9584\n",
      "Epoch 17 Test Loss: 0.4133 Test Acc: 0.8993\n",
      "Time usage 13m18s\n",
      "Epoch 18/20\n",
      "----------\n",
      "Epoch 18 Train G-mean为0.9452\n",
      "Epoch 18 Test G-mean为0.8130\n",
      "Epoch 18 Train Loss: 0.3732 Train Acc: 0.9510\n",
      "Epoch 18 Test Loss: 0.4852 Test Acc: 0.8190\n",
      "Time usage 14m4s\n",
      "Epoch 19/20\n",
      "----------\n",
      "Epoch 19 Train G-mean为0.9580\n",
      "Epoch 19 Test G-mean为0.6819\n",
      "Epoch 19 Train Loss: 0.3683 Train Acc: 0.9583\n",
      "Epoch 19 Test Loss: 0.5680 Test Acc: 0.7183\n",
      "Time usage 14m49s\n",
      "Epoch 20/20\n",
      "----------\n",
      "Epoch 20 Train G-mean为0.9658\n",
      "Epoch 20 Test G-mean为0.6945\n",
      "Epoch 20 Train Loss: 0.3611 Train Acc: 0.9644\n",
      "Epoch 20 Test Loss: 0.5674 Test Acc: 0.7257\n",
      "Time usage 15m46s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_acc_all</th>\n",
       "      <th>train_loss_all</th>\n",
       "      <th>val_acc_all</th>\n",
       "      <th>val_loss_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0.5771)</td>\n",
       "      <td>0.676716</td>\n",
       "      <td>tensor(0.5000)</td>\n",
       "      <td>0.789041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(0.7140)</td>\n",
       "      <td>0.579149</td>\n",
       "      <td>tensor(0.5056)</td>\n",
       "      <td>0.803770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tensor(0.7714)</td>\n",
       "      <td>0.523724</td>\n",
       "      <td>tensor(0.7743)</td>\n",
       "      <td>0.523982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tensor(0.8058)</td>\n",
       "      <td>0.496987</td>\n",
       "      <td>tensor(0.7034)</td>\n",
       "      <td>0.593486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tensor(0.8197)</td>\n",
       "      <td>0.483710</td>\n",
       "      <td>tensor(0.7015)</td>\n",
       "      <td>0.589894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>tensor(0.8243)</td>\n",
       "      <td>0.482452</td>\n",
       "      <td>tensor(0.7705)</td>\n",
       "      <td>0.515932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>tensor(0.8478)</td>\n",
       "      <td>0.464673</td>\n",
       "      <td>tensor(0.8377)</td>\n",
       "      <td>0.474090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>tensor(0.8543)</td>\n",
       "      <td>0.457988</td>\n",
       "      <td>tensor(0.8246)</td>\n",
       "      <td>0.471908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>tensor(0.8571)</td>\n",
       "      <td>0.456164</td>\n",
       "      <td>tensor(0.8134)</td>\n",
       "      <td>0.495670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>tensor(0.8750)</td>\n",
       "      <td>0.445166</td>\n",
       "      <td>tensor(0.7836)</td>\n",
       "      <td>0.517261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>tensor(0.8959)</td>\n",
       "      <td>0.429817</td>\n",
       "      <td>tensor(0.8246)</td>\n",
       "      <td>0.475822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.8910)</td>\n",
       "      <td>0.427776</td>\n",
       "      <td>tensor(0.7761)</td>\n",
       "      <td>0.512696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.9139)</td>\n",
       "      <td>0.408724</td>\n",
       "      <td>tensor(0.6381)</td>\n",
       "      <td>0.646824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>tensor(0.9235)</td>\n",
       "      <td>0.401327</td>\n",
       "      <td>tensor(0.7220)</td>\n",
       "      <td>0.571374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>tensor(0.9411)</td>\n",
       "      <td>0.386030</td>\n",
       "      <td>tensor(0.8377)</td>\n",
       "      <td>0.460012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>tensor(0.9459)</td>\n",
       "      <td>0.380866</td>\n",
       "      <td>tensor(0.9310)</td>\n",
       "      <td>0.398295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>tensor(0.9584)</td>\n",
       "      <td>0.367859</td>\n",
       "      <td>tensor(0.8993)</td>\n",
       "      <td>0.413265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>tensor(0.9510)</td>\n",
       "      <td>0.373172</td>\n",
       "      <td>tensor(0.8190)</td>\n",
       "      <td>0.485153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.9583)</td>\n",
       "      <td>0.368338</td>\n",
       "      <td>tensor(0.7183)</td>\n",
       "      <td>0.568046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>tensor(0.9644)</td>\n",
       "      <td>0.361121</td>\n",
       "      <td>tensor(0.7257)</td>\n",
       "      <td>0.567415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch   train_acc_all  train_loss_all     val_acc_all  val_loss_all\n",
       "0       0  tensor(0.5771)        0.676716  tensor(0.5000)      0.789041\n",
       "1       1  tensor(0.7140)        0.579149  tensor(0.5056)      0.803770\n",
       "2       2  tensor(0.7714)        0.523724  tensor(0.7743)      0.523982\n",
       "3       3  tensor(0.8058)        0.496987  tensor(0.7034)      0.593486\n",
       "4       4  tensor(0.8197)        0.483710  tensor(0.7015)      0.589894\n",
       "5       5  tensor(0.8243)        0.482452  tensor(0.7705)      0.515932\n",
       "6       6  tensor(0.8478)        0.464673  tensor(0.8377)      0.474090\n",
       "7       7  tensor(0.8543)        0.457988  tensor(0.8246)      0.471908\n",
       "8       8  tensor(0.8571)        0.456164  tensor(0.8134)      0.495670\n",
       "9       9  tensor(0.8750)        0.445166  tensor(0.7836)      0.517261\n",
       "10     10  tensor(0.8959)        0.429817  tensor(0.8246)      0.475822\n",
       "11     11  tensor(0.8910)        0.427776  tensor(0.7761)      0.512696\n",
       "12     12  tensor(0.9139)        0.408724  tensor(0.6381)      0.646824\n",
       "13     13  tensor(0.9235)        0.401327  tensor(0.7220)      0.571374\n",
       "14     14  tensor(0.9411)        0.386030  tensor(0.8377)      0.460012\n",
       "15     15  tensor(0.9459)        0.380866  tensor(0.9310)      0.398295\n",
       "16     16  tensor(0.9584)        0.367859  tensor(0.8993)      0.413265\n",
       "17     17  tensor(0.9510)        0.373172  tensor(0.8190)      0.485153\n",
       "18     18  tensor(0.9583)        0.368338  tensor(0.7183)      0.568046\n",
       "19     19  tensor(0.9644)        0.361121  tensor(0.7257)      0.567415"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_loader,test_loader = get_two_loader_from_1D_data(trainX,trainY,valX,valY)\n",
    "train_model_process(model,train_loader,test_loader,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
